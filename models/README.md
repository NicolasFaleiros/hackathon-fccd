# Projeto Detec√ß√£o de Fraudes - Hackathon - FCCD

Projeto de detec√ß√£o de fraude desenvolvido durante o curso de Forma√ß√£o Completa de Cientista de Dados baseado na competi√ß√£o Kaggle [Facebook Recruiting IV](https://www.kaggle.com/competitions/facebook-recruiting-iv-human-or-bot/overview). Link para competi√ß√£o FCCD [aqui](https://www.kaggle.com/competitions/hackathon-01-fccd). Dados originais dispon√≠veis no Kaggle para download. 
O modelo est√° otimizando a m√©trica Brier Score por ser a m√©trica alvo definida na competi√ß√£o. Nota final: 

## üí≠ Ideia utilizada

A ideia para desenvolvimento desse modelo foi criar padr√µes/features para identificar usu√°rios fraudulentos em vez de tentar identificar lances fraudulentos de forma separada. Al√©m disso, encontrar um padr√£o de dias na distribui√ß√£o de lances permitiu a divis√£o do comportamento dos usu√°rios dia-a-dia aumentando o tamanho de database de treino e melhorando o desempenho do modelo.

## ‚öôÔ∏è Features

As informa√ß√µes originais foram:

**Coluna** | **Explica√ß√£o**
----------- | ------------
id_lance | Identificador √∫nico do lance
id_participante | Identificador √∫nico do participante
leilao | Identificador √∫nico do leil√£o
mercadoria | A categoria da mercadoria leiloada
dispositivo | O dispositivo utilizado pelo visitante
tempo | O tempo que o lance foi feito
pais | O pa√≠s que o IP pertence
ip | O IP do participante
url | A URL de onde o participante foi referido


**Foram criadas 199 features no projeto, exemplos:**

* Features baseadas em m√©tricas das informa√ß√µes iniciais, como total, m√©dia, mediana de lances e de lances de um usu√°rio.
* Combina√ß√µes como Dispositivos por IP, URLs por leil√£o, entre outras.
* C√°lculos de entropia de informa√ß√µes em cada dia
* Features baseadas no tempo, como o quartil de tempo que o lance foi enviado, m√©dia/mediana de diferen√ßa de tempo entre lances e se um lance foi feito ou n√£o durante o pico do leil√£o 
* Ratios e produtos de features criadas. Foi feito um feature importance durante a cria√ß√£o do modelo e combinando (por meio de divis√£o ou produto) as features que mais influenciavam os resultados.

### **Import√¢ncia das features (maior para menor):**: 

As 10 primeiras:
feature | valor_importancia
--------|---------
mediana_lances_pordispositivo | 0.0951
media_urls_porleilao | 0.0395
ratio_media_mediana_dispositivos_porleilao | 0.0320
media_lances_pordispositivo | 0.0308
dispositivos_duplicados | 0.0267
porcentagem_lances_quartil_2 | 0.0221
media_dispositivos_porleilao | 0.0184
ratio_max_media_urls_porip | 0.0181
ips_duplicados | 0.0180
paises_duplicados | 0.0176


## Prepara√ß√£o de dados e Valida√ß√£o Cruzada

### Valida√ß√£o Cruzada

A valida√ß√£o cruzada utilizada foi StratifiedKFold da biblioteca sklearn.model_selection, com um total de 100 folds 

### Prepara√ß√£o de dados

Utilizando SimpleImputer da biblioteca sklearn.impute e StandardScaler de sklearn.preprocessing os dados foram preparados √† cada fold antes do treinamento

```
from sklearn.model_selection import StratifiedKFold
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


def preparar_dados(X, y, k):
    global imputer, scaler

    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    folds = []

    for fold_idx, (train_index, test_index) in enumerate(cv.split(X, y)):
        print(f"Preparando fold {fold_idx + 1}...")
        start_time = time.time()
        X_treino, X_teste = X[train_index], X[test_index]
        y_treino, y_teste = y[train_index], y[test_index]

        # Imputa√ß√£o de valores ausentes
        X_treino = imputer.fit_transform(X_treino)
        X_teste = imputer.transform(X_teste)

        # Aplica√ß√£o do StandardScaler nos dados de treino e teste
        X_treino = scaler.fit_transform(X_treino)
        X_teste = scaler.transform(X_teste)

        end_time = time.time()
        print(f"Tempo para preparar dados no fold {fold_idx + 1}: {end_time - start_time:.2f} segundos")
        folds.append((X_treino, y_treino, X_teste, y_teste))

    print("Prepara√ß√£o dos dados conclu√≠da.")
    return folds

```

## üìà Informa√ß√µes sobre algoritmo/hipertunagem/balanceamento

### Algoritmo utilizado:
Foi utilizado o classificador GradientBoostingClassifier da biblioteca sklearn.ensemble.
```
from sklearn.ensemble import GradientBoostingClassifier

modelo_final = GradientBoostingClassifier(
    max_depth=melhores_parametros['max_depth'],
    n_estimators=melhores_parametros['n_estimators'],
    learning_rate=melhores_parametros['learning_rate'],
    max_features=melhores_parametros['max_features'],
    min_samples_split=melhores_parametros['min_samples_split'],
    min_samples_leaf=melhores_parametros['min_samples_leaf'],
    random_state=0
)
```

### Hipertunagem
Os par√¢metros foram otimizados pela biblioteca por optuna utilizando esse range:
```
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),  
        'max_depth': trial.suggest_int('max_depth', 3, 15),  
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10), 
        'max_features': trial.suggest_float('max_features', 0.1, 1.0)  
    }

```

**par√¢metro** | **valor**
------------- | -----------
n_estimators | 143
learning_rate | 0.017876949339318857
max_depth | 12
min_samples_split | 19
min_samples_leaf | 10
max_features | 0.3444282296052409

### Balanceamento utilizado:
N√£o foi utilizada nenhuma t√©cnica espec√≠fica de balanceamento, dado que o algoritmo GradientBoostingClassifier j√° lida bem com dados desbalanceados.

## üîß Pr√©-requisitos

Acesse o arquivo requirements.txt do reposit√≥rio

## üìã Divis√£o do c√≥digo

Por quest√µes de capacidade computacional, foram divididas algumas partes do c√≥digo, como Feature Engineering.

A sequ√™ncia de execu√ß√£o para atingir os mesmos resultados √©:
1. divisao-lances-por-dia
2. feature-engineering-por-dia
3. juncao-df-dias
4. Hipertunagem com Optuna- GradientBoostClassifier
5. modelagem

## üì¶ Implanta√ß√£o

Ap√≥s execu√ß√£o do arquivo modelagem.py, um arquivo para submission ir√° ser criado na pasta arquivos_submission no formato 'resultado_GradientBoostingClassifier.csv_hipertunado_com_optuna_{k}folds.csv'

Modelo de READ.me criado por: [Armstrong Loh√£ns](https://gist.github.com/lohhans)
